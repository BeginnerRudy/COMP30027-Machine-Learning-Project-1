{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s):\n",
    "###### Python version:\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:50.966038Z",
     "start_time": "2019-03-22T03:05:50.955065Z"
=======
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.299560Z",
     "start_time": "2019-03-22T05:22:28.295571Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:50.982991Z",
     "start_time": "2019-03-22T03:05:50.969028Z"
=======
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.334466Z",
     "start_time": "2019-03-22T05:22:28.311587Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(filepath):\n",
    "    '''\n",
    "    Read file by filepath, and make it into a 2-D list\n",
    "    _____________________\n",
    "    Parameter:\n",
    "    filepath -> The filepath of dataset in csv format\n",
    "    ____________________\n",
    "    Return:\n",
    "    A 2-D list which contains the content of the dataset passed in\n",
    "    '''\n",
    "    # open the csv file in a reading mode\n",
    "    csv_file = open(filepath, 'r')\n",
    "    # The dataset for storing data\n",
    "    dataset = []\n",
    "    \n",
    "    # read the csv file line by line\n",
    "    for line in csv_file.readlines():\n",
    "        # Each line of the csv file represents an instance\n",
    "        instance = line.split(',')\n",
    "        # The split method above would capture the newline character '\\n' at end of the line\n",
    "        # Let's remove \"\\n\" by list slicing\n",
    "        instance[-1] = instance[-1][:-1]\n",
    "        # Append current instance to the final dataset\n",
    "        dataset.append(instance)\n",
    "    # Finishing reading, close the file\n",
    "    csv_file.close()\n",
    "    \n",
    "    # return the final dataset \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:50.998948Z",
     "start_time": "2019-03-22T03:05:50.985982Z"
=======
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.354415Z",
     "start_time": "2019-03-22T05:22:28.337458Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(dataset):\n",
    "    '''\n",
    "    Generate prior and posterir distribution from the give dataset.\n",
    "    The class atrribute is assumed to be at the last column of each instance. \n",
    "    _______________________________\n",
    "    Parameter:\n",
    "    dataset -> the trainning dataset in the form of 2-D list\n",
    "    _______________________________\n",
    "    Return:\n",
    "    prior -> A dictionary contains prior distribution of classes, that is P(cj)\n",
    "    posterior -> A dictionary contains posterior, that is P(xi|cj)\n",
    "    '''\n",
    "    return prior_calculator(dataset), posterior_calculator(dataset)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.012942Z",
     "start_time": "2019-03-22T03:05:51.001944Z"
=======
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.369373Z",
     "start_time": "2019-03-22T05:22:28.357408Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "def prior_calculator(dataset):\n",
    "    \"\"\"\n",
    "    Helper function for train(), responsible for calculating the prior of the given dataset\n",
    "    _____________________\n",
    "    Parameter:\n",
    "    dataset -> given dataset for calculating prior\n",
    "    _____________________\n",
    "    Return:\n",
    "    prior -> A dictionary represent the prior of the given dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    prior = {}\n",
    "    for instance in dataset:\n",
    "        the_class = instance[-1]\n",
    "        # if the class attribute is missing, simply ignore it\n",
    "        if the_class == '?':\n",
    "            pass\n",
    "         # if key is already seen before, just increase the count by 1\n",
    "        elif the_class in prior:\n",
    "            prior[the_class] += 1\n",
    "        # else create a new (key, value) pair and assign value to 1\n",
    "        else:\n",
    "            prior[the_class] = 1\n",
    "    \n",
    "    # divide the prior counts by total number to get probability\n",
    "    n = len(dataset)\n",
    "    for the_class in prior.keys():\n",
    "        prior[the_class] /= n\n",
    "    \n",
    "    # All done, return the prior\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.027902Z",
     "start_time": "2019-03-22T03:05:51.015903Z"
=======
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.387327Z",
     "start_time": "2019-03-22T05:22:28.372366Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "def posterior_calculator(dataset):\n",
    "    \"\"\"\n",
    "    This posterior follow the data structure disscussed in lecture\n",
    "    {attribute x: \n",
    "                {class c: \n",
    "                            {attribute_value: value}\n",
    "                }\n",
    "    }\n",
    "   \n",
    "    The posterior gives the probability after \"Laplace smoothing\", that is all count has a initial value of 1,\n",
    "    \n",
    "    P(xi|cj) = (1 + value_count)/(class_count + distinct_value_count_of_attribute)\n",
    "    \n",
    "    _________________________________\n",
    "    Parameter:\n",
    "    dataset -> The give dataset for calculating posterior distribution\n",
    "    _________________________________\n",
    "    Return:\n",
    "    posterior -> A dictionary represents the posterior distribution, that is P(xi|cj)\n",
    "    \"\"\"\n",
    "    # create the framework of the 3-D dictionary for posterior,   \n",
    "    posterior = create_posterior_framework_with_Lapalce_smoothing(dataset)\n",
    "    \n",
    "    \n",
    "    # total number of attributes\n",
    "    n = len(dataset[0]) - 1\n",
    "    \n",
    "    # count the attribute occurence instance by instance\n",
    "    for instance in dataset:\n",
    "        # is the class has not be included in the dictionary, add it in.\n",
    "        the_class = instance[-1]\n",
    "            \n",
    "        # feature means a attribute associated with value\n",
    "        # count feature according to its class and attribute belongs to\n",
    "        for attribute in range(n):\n",
    "            feature = instance[attribute]\n",
    "            # if the feature is missing, simply ignore it\n",
    "            if feature == '?':\n",
    "                pass\n",
    "            else:\n",
    "                posterior.get(attribute).get(the_class)[feature] += 1\n",
    "    \n",
    "    # do the last step of Laplace smoothing, divide attribtue count by (#class + # distinct attribute)\n",
    "    all_classes = posterior.get(0).keys()\n",
    "    \n",
    "    for attr in range(n):\n",
    "        for the_class in all_classes:\n",
    "            # since we set initial value of each distinct value count to 1, then if we sum up all distinct value\n",
    "            # then we actually get number_of_distinct_value + number_of_1, due to the initial value 1.\n",
    "            num_of_classes_plus_num_of_distinct_values = sum(posterior.get(attr).get(the_class).values())\n",
    "            for distinct_value in posterior.get(attr).get(the_class).keys():\n",
    "                posterior.get(attr).get(the_class)[distinct_value] /= num_of_classes_plus_num_of_distinct_values\n",
    "            \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.047817Z",
     "start_time": "2019-03-22T03:05:51.031860Z"
=======
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.404281Z",
     "start_time": "2019-03-22T05:22:28.390857Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "def create_posterior_framework_with_Lapalce_smoothing(dataset):\n",
    "    '''\n",
    "    Helper function of posterior_calculator.\n",
    "    This function intend to create a data structure shown as below:\n",
    "     {attribute x: \n",
    "                {class c: \n",
    "                            {attribute_value: 1}\n",
    "                }\n",
    "    }\n",
    "    \n",
    "    Note that the initial value of each attribute_value is 1, since I apply Laplace smoothing\n",
    "    _________________________\n",
    "    Parameter:\n",
    "    dataset-> given dataset\n",
    "    _________________________\n",
    "    Return:\n",
    "    posterior_framework -> The generated framework described above\n",
    "    '''\n",
    "    # the length of each instance => number of attributes in total\n",
    "    n = len(dataset[0])\n",
    "        \n",
    "    # find distinct values of all attributes, include the class attribute\n",
    "    # The data structure for storing information about distinct values is\n",
    "    #\n",
    "    # {attribute_1: [distinct_value_1, ..., distinct_value_n],\n",
    "    # ....\n",
    "    #  attribute_n: [distinct_value_1, ..., distinct_value_n],\n",
    "    #  }\n",
    "    \n",
    "    attribute_distinct_value = {}     \n",
    "    for i in range(n):\n",
    "        attribute_distinct_value[i] = []\n",
    "    \n",
    "    for instance in dataset:\n",
    "        for attr in range(n):\n",
    "            feature = instance[attr]\n",
    "            # if the attribute is missing, simply ignore it\n",
    "            if feature == '?':\n",
    "                pass\n",
    "            elif feature not in attribute_distinct_value.get(attr):\n",
    "                attribute_distinct_value[attr].append(feature)\n",
    "                \n",
    "                \n",
    "    # generate posterior_framework depends on attribute_distinct_value\n",
    "    num_attr = n - 1 # class attribute should be excluded\n",
    "    posterior_framework = {}     \n",
    "    for i in range(num_attr):\n",
    "        posterior_framework[i] = {}\n",
    "    \n",
    "    # class attributes processing\n",
    "    class_distinct_value = attribute_distinct_value.get(n-1)\n",
    "    \n",
    "    for attr in posterior_framework.keys():\n",
    "        for the_class in class_distinct_value:\n",
    "            posterior_framework.get(attr)[the_class] = {}\n",
    "    \n",
    "    # attribute processing\n",
    "    for attr in posterior_framework.keys():\n",
    "        for the_class in class_distinct_value:\n",
    "            for distinct_value in attribute_distinct_value.get(attr):\n",
    "                posterior_framework.get(attr).get(the_class)[distinct_value] = 1\n",
    "    \n",
    "    # All done!\n",
    "    return posterior_framework"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.060783Z",
     "start_time": "2019-03-22T03:05:51.051811Z"
=======
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.430212Z",
     "start_time": "2019-03-22T05:22:28.408003Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(test_set, prior, posterior):\n",
    "    '''\n",
    "    Predict the class label for each instance in test_set, the data structure of the result is:\n",
    "    [ prediction1, prediction2, ..., prediction_n]\n",
    "    If for some instance, there are several classes share the max probability, then predict the lable by \n",
    "    randomly choose one from these classes, this task is delegated to argmax(test_instance, prior, posterior).\n",
    "    ____________________________\n",
    "    Parameter:\n",
    "    test_set -> A 2D list contains test instances \n",
    "    prior -> Given prior distribution, which is a dictionary\n",
    "    posterior -> Given posterior distribution, which is a 3D dictionary.\n",
    "    ____________________________\n",
    "    Return:\n",
    "    test_results -> A list of results.\n",
    "    '''\n",
    "    # use arg max to find the class with highes probability of generating one test instance, \n",
    "    # and do it for each instance\n",
    "    test_results = []\n",
    "    for test_instance in test_set:\n",
    "        # arg max\n",
    "        test_results.append(argmax(test_instance, prior, posterior))\n",
    "        \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.074747Z",
     "start_time": "2019-03-22T03:05:51.062777Z"
=======
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.447165Z",
     "start_time": "2019-03-22T05:22:28.433203Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "def argmax(test_instance, prior, posterior):\n",
    "    '''\n",
    "    Find the class with largest probability for given test instance, prior and posterior.\n",
    "    If for some instance, there are several classes share the max probability, then predict the lable by \n",
    "    randomly choose one from these classes.\n",
    "    ______________________________________\n",
    "    Parameter:\n",
    "    test_set -> A 2D list contains test instances \n",
    "    prior -> Given prior distribution, which is a dictionary\n",
    "    posterior -> Given posterior distribution, which is a 3D dictionary.\n",
    "    ______________________________________\n",
    "    Return:\n",
    "    curr_argmax_class -> The class with max probability\n",
    "    '''\n",
    "    # get list of all possible class from prior\n",
    "    classes = list(prior.keys())\n",
    "    # I set this to a list, since there could be more than 1 class shre max probability\n",
    "    curr_argmax_class = [classes[0]]\n",
    "    curr_max_probability = probability_of_class_given_instance_calculator(test_instance, prior, posterior,\n",
    "                                                                             classes[0])\n",
    "    \n",
    "    for the_class in classes[1:]:\n",
    "        curr_prob = probability_of_class_given_instance_calculator(test_instance, prior, posterior,\n",
    "                                                                             the_class)\n",
    "        # handle class with same max probability\n",
    "        if curr_prob == curr_max_probability:\n",
    "            curr_argmax_class.append(the_class)\n",
    "        elif curr_prob > curr_max_probability:\n",
    "            curr_argmax_class = [the_class]\n",
    "    \n",
    "    # If there are more than class, choose one from them randomly\n",
    "    if len(curr_argmax_class) > 1:\n",
    "        curr_argmax_class = curr_argmax_class[random.randrange(0, len(curr_argmax_class)-1)]\n",
    "    # If there is only one, make it into a scalar, that is not a container.\n",
    "    else:\n",
    "        curr_argmax_class = curr_argmax_class[0]\n",
    "    return curr_argmax_class"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.087711Z",
     "start_time": "2019-03-22T03:05:51.077738Z"
=======
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.462125Z",
     "start_time": "2019-03-22T05:22:28.450157Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "def probability_of_class_given_instance_calculator(instance, prior, posterior, the_class):\n",
    "    '''\n",
    "    Calculat P(C=c|T=t) = prod(P(tj|C=c))*P(C = c)\n",
    "    Applying log transformation =>  log(P(C=c|T=t)) = log(sum(P(tj|C=c)) + log(P(C = c))\n",
    "    ________________________________________________\n",
    "    Parameter:\n",
    "    instance -> A test instances \n",
    "    prior -> Given prior distribution, which is a dictionary\n",
    "    posterior -> Given posterior distribution, which is a 3D dictionary.\n",
    "    the_class -> The given class\n",
    "    ________________________________________________\n",
    "    Return:\n",
    "     posterior_probablity*prior.get(the_class) -> prod(P(tj|C=c))*P(C = c)\n",
    "    '''\n",
    "    posterior_probablity = 1\n",
    "    for attr in range(len(instance) - 1):\n",
    "        feature = instance[attr]\n",
    "        # handle missing value by ignore it\n",
    "        if feature == \"?\":\n",
    "            pass\n",
    "        # Apply log transformation \n",
    "        else:\n",
    "            posterior_probablity += math.log(posterior.get(attr).get(the_class).get(feature))/math.log(2)\n",
    "    return posterior_probablity + math.log(prior.get(the_class))/math.log(2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.104665Z",
     "start_time": "2019-03-22T03:05:51.090702Z"
=======
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.477146Z",
     "start_time": "2019-03-22T05:22:28.465503Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate(test_label, predict_label):\n",
    "    '''\n",
    "    This evaluation applies the Accuracy metrix, it cares about how often the classifier get correct. \n",
    "    ___________________________________\n",
    "    Parameter:\n",
    "    test_label -> A list of actually label corresponds to each instance\n",
    "    predict_label -> A list of predicted label corresponds to each instance\n",
    "    ___________________________________\n",
    "    Return:\n",
    "    correct/n -> Accuracy, which is how of the classifier get correct.\n",
    "    '''\n",
    "    correct = 0\n",
    "    n = len(test_label)\n",
    "    for i in range(n):\n",
    "        if test_label[i] == predict_label[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct/n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:06:31.346291Z",
     "start_time": "2019-03-22T03:06:31.335289Z"
=======
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.494100Z",
     "start_time": "2019-03-22T05:22:28.480535Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(dataset):\n",
    "    '''\n",
    "    Calculate the information gain of each attribtues with respect to the class distribution\n",
    "    _____________________________\n",
    "    Parameter:\n",
    "    dataset -> the given dataset for calculating information gain\n",
    "    _____________________________\n",
    "    Return:\n",
    "    IG -> A dictionary contains the final information gain.\n",
    "    '''\n",
    "    \n",
    "    IG = {}\n",
    "    Entropy_Of_Attributes = {}\n",
    "    Entropy_Of_Attributes_Given_Class = {}\n",
    "    # generate a list of int represents for attributes\n",
    "    attributes = list(range(len(dataset[0][:-1])))\n",
    "    \n",
    "    # get distinct value of classes, and counting how many of each are ther\n",
    "    classes = {}\n",
    "    for instance in dataset:\n",
    "        the_class = instance[-1]\n",
    "        if the_class == '?':\n",
    "            pass\n",
    "        elif the_class not in classes:\n",
    "            classes[the_class] = 1\n",
    "        else:\n",
    "            classes[the_class] += 1\n",
    "    \n",
    "    # initialize the 3 dictionaries, given each attribute initial value of 0.\n",
    "    for attr in attributes:\n",
    "        IG[attr] = 0\n",
    "        Entropy_Of_Attributes[attr] = 0\n",
    "        Entropy_Of_Attributes_Given_Class[attr] = 0\n",
    "    \n",
    "    # Calculate the entropy of each attribute\n",
    "    for attr in attributes:\n",
    "        distribution = get_attribute_distribution(dataset, attr)\n",
    "        Entropy_Of_Attributes[attr] = H(distribution)\n",
    "    \n",
    "    # Calculate the entropy given the class\n",
    "    class_count = sum(list(classes.values()))\n",
    "    for attr in attributes:\n",
    "        mean_info = 0\n",
    "        for the_class in list(classes.keys()):\n",
    "            # Partition the class with respect to the given class\n",
    "            dataset_given_class = filter_dataset_by_class(dataset, the_class)\n",
    "            distribution_of_attr_given_ = get_attribute_distribution(dataset_given_class, attr)\n",
<<<<<<< HEAD
    "            new_entropy = H(new_distribution_of_attr)\n",
=======
    "            new_entropy = H(distribution_of_attr_given_)\n",
>>>>>>> add_log_transformation
    "            mean_info += new_entropy*(classes.get(the_class)/class_count)\n",
    "        Entropy_Of_Attributes_Given_Class[attr] = mean_info\n",
    "        \n",
    "        \n",
    "    # Calculate IG by H(attribtue) - H(attribute|Class) = Information Gain\n",
    "    for attr in attributes:\n",
    "        IG[attr] = Entropy_Of_Attributes.get(attr) - Entropy_Of_Attributes_Given_Class.get(attr)\n",
    "    \n",
    "    \n",
    "    return IG"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.132592Z",
     "start_time": "2019-03-22T03:05:51.123617Z"
=======
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.515981Z",
     "start_time": "2019-03-22T05:22:28.498030Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "def get_attribute_distribution(dataset, attr):\n",
    "    '''\n",
    "    Get the column belongs to the attr and make it into a list\n",
    "    ______________________________________\n",
    "    Parameter:\n",
    "    dataset -> Given dataset\n",
    "    attr -> Given attr to get\n",
    "    ______________________________________\n",
    "    Return:\n",
    "    distribution -> A list of attr values\n",
    "    '''\n",
    "    distribution = []\n",
    "    for instance in dataset:\n",
    "        distribution.append(instance[attr])\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.145556Z",
     "start_time": "2019-03-22T03:05:51.135583Z"
=======
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.530941Z",
     "start_time": "2019-03-22T05:22:28.519971Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "def H(distribution):\n",
    "    '''\n",
    "    Calculate the entropy of the distribution by H(X) = -1 * sum(P(X = x)*log2(P(X = x)))\n",
    "    __________________________\n",
    "    Paramater:\n",
    "    distribution -> the given distribution for calculating\n",
    "    __________________________\n",
    "    Return:\n",
    "    entropy -> the result of entropy\n",
    "    '''\n",
    "    # Find all distinct value and Count number of occurence of each distinct value\n",
    "    distinct_values = {}     \n",
    "    total_value_count = 0\n",
    "    for value in distribution:\n",
    "        # if the attribute is missing, simply ignore it\n",
    "        if value == '?':\n",
    "            pass\n",
    "        elif value in distinct_values:\n",
    "            distinct_values[value] += 1\n",
    "            total_value_count += 1\n",
    "        elif value not in distinct_values:\n",
    "            distinct_values[value] = 1\n",
    "            total_value_count += 1\n",
    "        \n",
    "    # Divide by total number to get probability and  Calculate the entropy\n",
    "    entropy = 0\n",
    "    for value in list(distinct_values.keys()):\n",
    "        distinct_values[value] /= total_value_count\n",
    "        entropy += -1 * distinct_values[value] * (math.log(distinct_values[value])/math.log(2))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.159519Z",
     "start_time": "2019-03-22T03:05:51.148547Z"
=======
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.545901Z",
     "start_time": "2019-03-22T05:22:28.533933Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [],
   "source": [
    "def filter_dataset_by_class(dataset, the_class):\n",
    "    '''\n",
    "    Filter the dataset by given class, and return the result\n",
    "    _________________________________\n",
    "    Parameter:\n",
    "    dataset -> Given dataset to filter\n",
    "    the_class -> The class use to filter\n",
    "    _________________________________\n",
    "    Return:\n",
    "    new_data_set -> the dataset only contains instances with the_class label\n",
    "    '''\n",
    "    new_data_set = []\n",
    "    for instance in  dataset:\n",
    "        if instance[-1] == the_class:\n",
    "            new_data_set.append(instance)\n",
    "    return new_data_set"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.209384Z",
     "start_time": "2019-03-22T03:05:51.162510Z"
=======
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.596844Z",
     "start_time": "2019-03-22T05:22:28.548894Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6763005780346821"
      ]
     },
<<<<<<< HEAD
     "execution_count": 15,
=======
     "execution_count": 162,
>>>>>>> add_log_transformation
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset by index\n",
    "car = preprocess(\"./2019S1-proj1-data_dos/car.csv\")\n",
    "split_number = round(len(car)*0.8)\n",
    "training_set = car[:split_number]\n",
    "test_set = car[split_number:]\n",
    "prior, posterior = train(training_set)\n",
    "test_label = [test_instance[-1] for test_instance in test_set]\n",
    "predict_labels = predict(test_set, prior, posterior)\n",
    "evaluate(test_label, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T11:37:04.006074Z",
     "start_time": "2019-03-24T11:37:03.857731Z"
=======
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.710461Z",
     "start_time": "2019-03-22T05:22:28.599783Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40412979351032446"
      ]
     },
<<<<<<< HEAD
     "execution_count": 68,
=======
     "execution_count": 163,
>>>>>>> add_log_transformation
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the whole dataset as both training and test set\n",
    "car = preprocess(\"./2019S1-proj1-data_dos/car.csv\")\n",
    "prior, posterior = train(car)\n",
    "test_label = [test_instance[-1] for test_instance in car]\n",
    "predict_labels = predict(car, prior, posterior)\n",
    "evaluate(test_label, predict_labels)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:06:34.531141Z",
     "start_time": "2019-03-22T03:06:34.511192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.09644896916961399,\n",
       " 1: 0.07370394692148596,\n",
       " 2: 0.00448571662663233,\n",
       " 3: 0.21966296333990787,\n",
       " 4: 0.030008141247605202,\n",
       " 5: 0.26218435655426364}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gain(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.287178Z",
     "start_time": "2019-03-22T03:05:51.278202Z"
=======
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.740456Z",
     "start_time": "2019-03-22T05:22:28.713573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.09644896916961399,\n",
       " 1: 0.07370394692148596,\n",
       " 2: 0.00448571662663233,\n",
       " 3: 0.21966296333990787,\n",
       " 4: 0.030008141247605202,\n",
       " 5: 0.26218435655426364}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gain(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.759330Z",
     "start_time": "2019-03-22T05:22:28.743432Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unacc', 'acc', 'vgood', 'good']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 17,
=======
     "execution_count": 165,
>>>>>>> add_log_transformation
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(prior.keys())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.311114Z",
     "start_time": "2019-03-22T03:05:51.291168Z"
=======
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.786259Z",
     "start_time": "2019-03-22T05:22:28.762323Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " 'acc',\n",
       " 'acc',\n",
       " 'unacc',\n",
       " ...]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 18,
=======
     "execution_count": 166,
>>>>>>> add_log_transformation
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_labels"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.325076Z",
     "start_time": "2019-03-22T03:05:51.315103Z"
=======
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.801232Z",
     "start_time": "2019-03-22T05:22:28.789252Z"
>>>>>>> add_log_transformation
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unacc': 0.7002314814814815,\n",
       " 'acc': 0.2222222222222222,\n",
       " 'vgood': 0.03761574074074074,\n",
       " 'good': 0.03993055555555555}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 19,
=======
     "execution_count": 167,
>>>>>>> add_log_transformation
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T03:05:51.342033Z",
     "start_time": "2019-03-22T03:05:51.327070Z"
    }
=======
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:22:28.820170Z",
     "start_time": "2019-03-22T05:22:28.804212Z"
    },
    "scrolled": true
>>>>>>> add_log_transformation
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'unacc': {'vhigh': 0.29736408566721584,\n",
       "   'high': 0.2677100494233937,\n",
       "   'med': 0.2215815485996705,\n",
       "   'low': 0.21334431630971992},\n",
       "  'acc': {'vhigh': 0.18814432989690721,\n",
       "   'high': 0.2809278350515464,\n",
       "   'med': 0.29896907216494845,\n",
       "   'low': 0.23195876288659795},\n",
       "  'vgood': {'vhigh': 0.014492753623188406,\n",
       "   'high': 0.014492753623188406,\n",
       "   'med': 0.391304347826087,\n",
       "   'low': 0.5797101449275363},\n",
       "  'good': {'vhigh': 0.0136986301369863,\n",
       "   'high': 0.0136986301369863,\n",
       "   'med': 0.3287671232876712,\n",
       "   'low': 0.6438356164383562}},\n",
       " 1: {'unacc': {'vhigh': 0.29736408566721584,\n",
       "   'high': 0.25947281713344317,\n",
       "   'med': 0.2215815485996705,\n",
       "   'low': 0.2215815485996705},\n",
       "  'acc': {'vhigh': 0.18814432989690721,\n",
       "   'high': 0.27319587628865977,\n",
       "   'med': 0.29896907216494845,\n",
       "   'low': 0.23969072164948454},\n",
       "  'vgood': {'vhigh': 0.014492753623188406,\n",
       "   'high': 0.2028985507246377,\n",
       "   'med': 0.391304347826087,\n",
       "   'low': 0.391304347826087},\n",
       "  'good': {'vhigh': 0.0136986301369863,\n",
       "   'high': 0.0136986301369863,\n",
       "   'med': 0.3287671232876712,\n",
       "   'low': 0.6438356164383562}},\n",
       " 2: {'unacc': {'2': 0.26935749588138386,\n",
       "   '3': 0.24794069192751236,\n",
       "   '4': 0.2413509060955519,\n",
       "   '5more': 0.2413509060955519},\n",
       "  'acc': {'2': 0.211340206185567,\n",
       "   '3': 0.25773195876288657,\n",
       "   '4': 0.2654639175257732,\n",
       "   '5more': 0.2654639175257732},\n",
       "  'vgood': {'2': 0.15942028985507245,\n",
       "   '3': 0.2318840579710145,\n",
       "   '4': 0.30434782608695654,\n",
       "   '5more': 0.30434782608695654},\n",
       "  'good': {'2': 0.2191780821917808,\n",
       "   '3': 0.2602739726027397,\n",
       "   '4': 0.2602739726027397,\n",
       "   '5more': 0.2602739726027397}},\n",
       " 3: {'unacc': {'2': 0.4756801319043693,\n",
       "   '4': 0.258037922506183,\n",
       "   'more': 0.2662819455894477},\n",
       "  'acc': {'2': 0.002583979328165375,\n",
       "   '4': 0.5142118863049095,\n",
       "   'more': 0.48320413436692505},\n",
       "  'vgood': {'2': 0.014705882352941176,\n",
       "   '4': 0.45588235294117646,\n",
       "   'more': 0.5294117647058824},\n",
       "  'good': {'2': 0.013888888888888888,\n",
       "   '4': 0.5138888888888888,\n",
       "   'more': 0.4722222222222222}},\n",
       " 4: {'unacc': {'small': 0.37180544105523494,\n",
       "   'med': 0.3239901071723001,\n",
       "   'big': 0.30420445177246497},\n",
       "  'acc': {'small': 0.2739018087855297,\n",
       "   'med': 0.35142118863049093,\n",
       "   'big': 0.37467700258397935},\n",
       "  'vgood': {'small': 0.014705882352941176,\n",
       "   'med': 0.38235294117647056,\n",
       "   'big': 0.6029411764705882},\n",
       "  'good': {'small': 0.3055555555555556,\n",
       "   'med': 0.3472222222222222,\n",
       "   'big': 0.3472222222222222}},\n",
       " 5: {'unacc': {'low': 0.4756801319043693,\n",
       "   'med': 0.2951360263808739,\n",
       "   'high': 0.2291838417147568},\n",
       "  'acc': {'low': 0.002583979328165375,\n",
       "   'med': 0.46770025839793283,\n",
       "   'high': 0.5297157622739018},\n",
       "  'vgood': {'low': 0.014705882352941176,\n",
       "   'med': 0.014705882352941176,\n",
       "   'high': 0.9705882352941176},\n",
       "  'good': {'low': 0.013888888888888888,\n",
       "   'med': 0.5555555555555556,\n",
       "   'high': 0.4305555555555556}}}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 20,
=======
     "execution_count": 168,
>>>>>>> add_log_transformation
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution  does this help to explain the classifiers behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a Decision Stump (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a holdout or crossvalidation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:26:15.243909Z",
     "start_time": "2019-03-22T05:26:15.238923Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ['anneal', 'breast-cancer', 'car', 'cmc', 'hepatitis', 'hypothyroid', 'mushroom', 'nursery',\n",
    "           'primary-tumor']\n",
    "root_path = './2019S1-proj1-data_dos'\n",
    "extension = 'csv'\n",
    "file_paths = [root_path + name + extension for name in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:34:13.972439Z",
     "start_time": "2019-03-22T05:34:13.966518Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_of_NB_classifier(file_paths):\n",
    "    accuracy_dict = {}\n",
    "    for file_path in file_paths:\n",
    "        dataset_name = file_path.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        dataset =  preprocess(file_path)\n",
    "        prior, posterior = train(dataset)\n",
    "        actual_labels = [instance[-1] for instance in dataset]\n",
    "        predict_labels = predict(dataset, prior, posterior)\n",
    "        accuracy = evaluate(predict_labels, actual_labels)\n",
    "        \n",
    "        accuracy_dict[dataset_name] = accuracy\n",
    "    return accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:39:41.442716Z",
     "start_time": "2019-03-22T05:39:41.436732Z"
    }
   },
   "outputs": [],
   "source": [
    "def info_gain_of_datasets(file_paths):\n",
    "    info_gain_dict = {}\n",
    "    for file_path in file_paths:\n",
    "        dataset_name = file_path.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        dataset =  preprocess(file_path)\n",
    "        ig = info_gain(dataset)\n",
    "        \n",
    "        info_gain_dict[dataset_name] = ig\n",
    "    return info_gain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:39:46.433909Z",
     "start_time": "2019-03-22T05:39:42.246579Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_dict = accuracy_of_NB_classifier(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:39:47.990515Z",
     "start_time": "2019-03-22T05:39:46.437665Z"
    }
   },
   "outputs": [],
   "source": [
    "ig_dict = info_gain_of_datasets(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T06:00:09.295330Z",
     "start_time": "2019-03-22T06:00:09.289420Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_report(acc_dict):\n",
    "    print(\"The accuracy table:\")\n",
    "    for dataset in list(acc_dict.keys()):\n",
    "        print(dataset + \" \"+ str(acc_dict.get(dataset)))\n",
    "        \n",
    "    print('\\n')\n",
    "    accuracy_list = list(acc_dict.values())\n",
    "    print(\"The mean accuracy is \" + str())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T05:59:13.597829Z",
     "start_time": "2019-03-22T05:59:13.592197Z"
    }
   },
   "outputs": [],
   "source": [
    "def report(acc_dict, ig_dict):\n",
    "    sepetator = \"\\n********************************************************************\\n\"\n",
    "    for dataset in list(acc_dict.keys()):\n",
    "        info_gain_distribution = (ig_dict.get(dataset).values())\n",
    "        print(\"Accuracy of \" + dataset + str(acc_dict.get(dataset)))\n",
    "        print(\"The max information gain among all attributes is \" + str(max(info_gain_distribution)))\n",
    "        print(sepetator)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T06:00:16.705098Z",
     "start_time": "2019-03-22T06:00:16.699154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy table:\n",
      "anneal 0.8930957683741648\n",
      "breast-cancer 0.7482517482517482\n",
      "car 0.8784722222222222\n",
      "cmc 0.47997284453496264\n",
      "hepatitis 0.832258064516129\n",
      "hypothyroid 0.9519443566234588\n",
      "mushroom 0.9588872476612507\n",
      "nursery 0.34930555555555554\n",
      "primary-tumor 0.40412979351032446\n"
     ]
    }
   ],
   "source": [
    "accuracy_report(acc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T06:00:12.363220Z",
     "start_time": "2019-03-22T06:00:12.356185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of anneal0.8930957683741648\n",
      "The max information gain among all attributes is 0.43517783626288575\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Accuracy of breast-cancer0.7482517482517482\n",
      "The max information gain among all attributes is 0.0770098525166143\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Accuracy of car0.8784722222222222\n",
      "The max information gain among all attributes is 0.26218435655426364\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Accuracy of cmc0.47997284453496264\n",
      "The max information gain among all attributes is 0.10173991727554066\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Accuracy of hepatitis0.832258064516129\n",
      "The max information gain among all attributes is 0.1327707012958565\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Accuracy of hypothyroid0.9519443566234588\n",
      "The max information gain among all attributes is 0.00935371021558018\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Accuracy of mushroom0.9588872476612507\n",
      "The max information gain among all attributes is 0.9060749773839993\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Accuracy of nursery0.34930555555555554\n",
      "The max information gain among all attributes is 0.9587749604699761\n",
      "\n",
      "********************************************************************\n",
      "\n",
      "Accuracy of primary-tumor0.40412979351032446\n",
      "The max information gain among all attributes is 0.49984770199247985\n",
      "\n",
      "********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(acc_dict, ig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
