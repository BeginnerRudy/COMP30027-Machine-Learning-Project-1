{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s):\n",
    "###### Python version:\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T03:27:27.701247Z",
     "start_time": "2019-03-18T03:27:27.694798Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(filepath):\n",
    "    '''\n",
    "    Read file by filepath, and make it into a 2-D list\n",
    "    _____________________\n",
    "    Parameter:\n",
    "    filepath -> The filepath of dataset in csv format\n",
    "    ____________________\n",
    "    Return:\n",
    "    A 2-D list which contains the content of the dataset passed in\n",
    "    '''\n",
    "    # open the csv file in a reading mode\n",
    "    csv_file = open(filepath, 'r')\n",
    "    # The dataset for storing data\n",
    "    dataset = []\n",
    "    \n",
    "    # read the csv file line by line\n",
    "    for line in csv_file.readlines():\n",
    "        # Each line of the csv file represents an instance\n",
    "        instance = line.split(',')\n",
    "        # The split method above would capture the newline character '\\n' at end of the line\n",
    "        # Let's remove \"\\n\" by list slicing\n",
    "        instance[-1] = instance[-1][:-1]\n",
    "        # Append current instance to the final dataset\n",
    "        dataset.append(instance)\n",
    "    # Finishing reading, close the file\n",
    "    csv_file.close()\n",
    "    \n",
    "    # return the final dataset \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T03:54:21.880858Z",
     "start_time": "2019-03-18T03:54:21.876394Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(dataset):\n",
    "    '''\n",
    "    Generate prior and posterir distribution from the give dataset.\n",
    "    The class atrribute is assumed to be at the last column of each instance. \n",
    "    _______________________________\n",
    "    Parameter:\n",
    "    dataset -> the trainning dataset in the form of 2-D list\n",
    "    _______________________________\n",
    "    Return:\n",
    "    prior -> A dictionary contains prior distribution of classes, that is P(cj)\n",
    "    posterior -> A dictionary contains posterior, that is P(xi|cj)\n",
    "    '''\n",
    "    return prior_calculator(dataset), posterior_calculator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_calculator(dataset):\n",
    "    \"\"\"\n",
    "    Helper function for train(), responsible for calculating the prior of the given dataset\n",
    "    _____________________\n",
    "    Parameter:\n",
    "    dataset -> given dataset for calculating prior\n",
    "    _____________________\n",
    "    Return:\n",
    "    prior -> A dictionary represent the prior of the given dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    prior = {}\n",
    "    for instance in dataset:\n",
    "        the_class = instance[-1]\n",
    "         # if key is already seen before, just increase the count by 1\n",
    "        if the_class in prior:\n",
    "            prior[the_class] += 1\n",
    "        # else create a new (key, value) pair and assign value to 1\n",
    "        else:\n",
    "            prior[the_class] = 1\n",
    "    \n",
    "    # divide the prior counts by total number to get probability\n",
    "    n = len(dataset)\n",
    "    for the_class in prior.keys():\n",
    "        prior[the_class] /= n\n",
    "    \n",
    "    # All done, return the prior\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_calculator(dataset):\n",
    "    \"\"\"\n",
    "    This posterior follow the data structure disscussed in lecture\n",
    "    {attribute x: \n",
    "                {class c: \n",
    "                            {attribute_value: value}\n",
    "                }\n",
    "    }\n",
    "   \n",
    "    The posterior gives the probability after \"Laplace smoothing\", that is all count has a initial value of 1,\n",
    "    \n",
    "    P(xi|cj) = (1 + value_count)/(class_count + distinct_value_count_of_attribute)\n",
    "    \n",
    "    _________________________________\n",
    "    Parameter:\n",
    "    dataset -> The give dataset for calculating posterior distribution\n",
    "    _________________________________\n",
    "    Return:\n",
    "    posterior -> A dictionary represents the posterior distribution, that is P(xi|cj)\n",
    "    \"\"\"\n",
    "    # create the framework of the 3-D dictionary for posterior,   \n",
    "    posterior = create_posterior_framework_with_Lapalce_smoothing(dataset)\n",
    "    \n",
    "    \n",
    "    # total number of attributes\n",
    "    n = len(dataset[0]) - 1\n",
    "    \n",
    "    # count the attribute occurence instance by instance\n",
    "    for instance in dataset:\n",
    "        # is the class has not be included in the dictionary, add it in.\n",
    "        the_class = instance[-1]\n",
    "            \n",
    "        # feature means a attribute associated with value\n",
    "        # count feature according to its class and attribute belongs to\n",
    "        for attribute in range(n):\n",
    "            feature = instance[attribute]\n",
    "            posterior.get(attribute).get(the_class)[feature] += 1\n",
    "    \n",
    "    # do the last step of Laplace smoothing, divide attribtue count by (#class + # distinct attribute)\n",
    "    all_classes = posterior.get(0).keys()\n",
    "    \n",
    "    for attr in range(n):\n",
    "        for the_class in all_classes:\n",
    "            # since we set initial value of each distinct value count to 1, then if we sum up all distinct value\n",
    "            # then we actually get number_of_distinct_value + number_of_1, due to the initial value 1.\n",
    "            num_of_classes_plus_num_of_distinct_values = sum(posterior.get(attr).get(the_class).values())\n",
    "            for distinct_value in posterior.get(attr).get(the_class).keys():\n",
    "                posterior.get(attr).get(the_class)[distinct_value] /= num_of_classes_plus_num_of_distinct_values\n",
    "            \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_posterior_framework_with_Lapalce_smoothing(dataset):\n",
    "    '''\n",
    "    Helper function of posterior_calculator.\n",
    "    This function intend to create a data structure shown as below:\n",
    "     {attribute x: \n",
    "                {class c: \n",
    "                            {attribute_value: 1}\n",
    "                }\n",
    "    }\n",
    "    \n",
    "    Note that the initial value of each attribute_value is 1, since I apply Laplace smoothing\n",
    "    _________________________\n",
    "    Parameter:\n",
    "    dataset-> given dataset\n",
    "    _________________________\n",
    "    Return:\n",
    "    posterior_framework -> The generated framework described above\n",
    "    '''\n",
    "    # the length of each instance => number of attributes in total\n",
    "    n = len(dataset[0])\n",
    "        \n",
    "    # find distinct values of all attributes, include the class attribute\n",
    "    # The data structure for storing information about distinct values is\n",
    "    #\n",
    "    # {attribute_1: [distinct_value_1, ..., distinct_value_n],\n",
    "    # ....\n",
    "    #  attribute_n: [distinct_value_1, ..., distinct_value_n],\n",
    "    #  }\n",
    "    \n",
    "    attribute_distinct_value = {}     \n",
    "    for i in range(n):\n",
    "        attribute_distinct_value[i] = []\n",
    "    \n",
    "    for instance in dataset:\n",
    "        for attr in range(n):\n",
    "            feature = instance[attr]\n",
    "            if feature not in attribute_distinct_value.get(attr):\n",
    "                attribute_distinct_value[attr].append(feature)\n",
    "                \n",
    "                \n",
    "    # generate posterior_framework depends on attribute_distinct_value\n",
    "    num_attr = n - 1 # class attribute should be excluded\n",
    "    posterior_framework = {}     \n",
    "    for i in range(num_attr):\n",
    "        posterior_framework[i] = {}\n",
    "    \n",
    "    # class attributes processing\n",
    "    class_distinct_value = attribute_distinct_value.get(n-1)\n",
    "    \n",
    "    for attr in posterior_framework.keys():\n",
    "        for the_class in class_distinct_value:\n",
    "            posterior_framework.get(attr)[the_class] = {}\n",
    "    \n",
    "    # attribute processing\n",
    "    for attr in posterior_framework.keys():\n",
    "        for the_class in class_distinct_value:\n",
    "            for distinct_value in attribute_distinct_value.get(attr):\n",
    "                posterior_framework.get(attr).get(the_class)[distinct_value] = 1\n",
    "    \n",
    "    # All done!\n",
    "    return posterior_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(test_set, prior, posterior):\n",
    "    # use arg max to find the class with highes probability of generating one test instance, \n",
    "    # and do it for each instance\n",
    "    for test_instance in test_set:\n",
    "        # arg max\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(test_instance, prior, posterior):\n",
    "    # get list of all possible class from prior\n",
    "    classes = prior.keys()\n",
    "    # I set this to a list, since there could be more than 1 class shre max probability\n",
    "    curr_argmax_class = [classes[0]]\n",
    "    curr_argmax_probability = probability_of_class_given_instance_calculator(test_instance, prior, posterior,\n",
    "                                                                             classes[0])\n",
    "    \n",
    "    for the_class in classes[1:]:\n",
    "        # handle class with same max probability\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T11:28:19.660923Z",
     "start_time": "2019-03-19T11:28:19.647818Z"
    }
   },
   "outputs": [],
   "source": [
    "def probability_of_class_given_instance_calculator(instance, prior, posterior, the_class):\n",
    "    posterior_probablity = 1\n",
    "    for attr in range(len(instance) - 1):\n",
    "        feature = instance[attr]\n",
    "        # handle missing value by ignore it\n",
    "        if feature == \"?\":\n",
    "            pass\n",
    "        else:\n",
    "            posterior_probablity *= posterior.get(attr).get(the_class).get(feature)\n",
    "    return posterior_probablity*prior.get(the_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T11:09:44.988614Z",
     "start_time": "2019-03-19T11:09:44.959826Z"
    }
   },
   "outputs": [],
   "source": [
    "car = preprocess(\"./2019S1-proj1-data_dos/car.csv\")\n",
    "prior, posterior = train(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T11:15:08.349425Z",
     "start_time": "2019-03-19T11:15:08.343968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['unacc', 'acc', 'vgood', 'good'])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T11:09:56.140662Z",
     "start_time": "2019-03-19T11:09:56.128262Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'unacc': {'vhigh': 0.29736408566721584,\n",
       "   'high': 0.2677100494233937,\n",
       "   'med': 0.2215815485996705,\n",
       "   'low': 0.21334431630971992},\n",
       "  'acc': {'vhigh': 0.18814432989690721,\n",
       "   'high': 0.2809278350515464,\n",
       "   'med': 0.29896907216494845,\n",
       "   'low': 0.23195876288659795},\n",
       "  'vgood': {'vhigh': 0.014492753623188406,\n",
       "   'high': 0.014492753623188406,\n",
       "   'med': 0.391304347826087,\n",
       "   'low': 0.5797101449275363},\n",
       "  'good': {'vhigh': 0.0136986301369863,\n",
       "   'high': 0.0136986301369863,\n",
       "   'med': 0.3287671232876712,\n",
       "   'low': 0.6438356164383562}},\n",
       " 1: {'unacc': {'vhigh': 0.29736408566721584,\n",
       "   'high': 0.25947281713344317,\n",
       "   'med': 0.2215815485996705,\n",
       "   'low': 0.2215815485996705},\n",
       "  'acc': {'vhigh': 0.18814432989690721,\n",
       "   'high': 0.27319587628865977,\n",
       "   'med': 0.29896907216494845,\n",
       "   'low': 0.23969072164948454},\n",
       "  'vgood': {'vhigh': 0.014492753623188406,\n",
       "   'high': 0.2028985507246377,\n",
       "   'med': 0.391304347826087,\n",
       "   'low': 0.391304347826087},\n",
       "  'good': {'vhigh': 0.0136986301369863,\n",
       "   'high': 0.0136986301369863,\n",
       "   'med': 0.3287671232876712,\n",
       "   'low': 0.6438356164383562}},\n",
       " 2: {'unacc': {'2': 0.26935749588138386,\n",
       "   '3': 0.24794069192751236,\n",
       "   '4': 0.2413509060955519,\n",
       "   '5more': 0.2413509060955519},\n",
       "  'acc': {'2': 0.211340206185567,\n",
       "   '3': 0.25773195876288657,\n",
       "   '4': 0.2654639175257732,\n",
       "   '5more': 0.2654639175257732},\n",
       "  'vgood': {'2': 0.15942028985507245,\n",
       "   '3': 0.2318840579710145,\n",
       "   '4': 0.30434782608695654,\n",
       "   '5more': 0.30434782608695654},\n",
       "  'good': {'2': 0.2191780821917808,\n",
       "   '3': 0.2602739726027397,\n",
       "   '4': 0.2602739726027397,\n",
       "   '5more': 0.2602739726027397}},\n",
       " 3: {'unacc': {'2': 0.4756801319043693,\n",
       "   '4': 0.258037922506183,\n",
       "   'more': 0.2662819455894477},\n",
       "  'acc': {'2': 0.002583979328165375,\n",
       "   '4': 0.5142118863049095,\n",
       "   'more': 0.48320413436692505},\n",
       "  'vgood': {'2': 0.014705882352941176,\n",
       "   '4': 0.45588235294117646,\n",
       "   'more': 0.5294117647058824},\n",
       "  'good': {'2': 0.013888888888888888,\n",
       "   '4': 0.5138888888888888,\n",
       "   'more': 0.4722222222222222}},\n",
       " 4: {'unacc': {'small': 0.37180544105523494,\n",
       "   'med': 0.3239901071723001,\n",
       "   'big': 0.30420445177246497},\n",
       "  'acc': {'small': 0.2739018087855297,\n",
       "   'med': 0.35142118863049093,\n",
       "   'big': 0.37467700258397935},\n",
       "  'vgood': {'small': 0.014705882352941176,\n",
       "   'med': 0.38235294117647056,\n",
       "   'big': 0.6029411764705882},\n",
       "  'good': {'small': 0.3055555555555556,\n",
       "   'med': 0.3472222222222222,\n",
       "   'big': 0.3472222222222222}},\n",
       " 5: {'unacc': {'low': 0.4756801319043693,\n",
       "   'med': 0.2951360263808739,\n",
       "   'high': 0.2291838417147568},\n",
       "  'acc': {'low': 0.002583979328165375,\n",
       "   'med': 0.46770025839793283,\n",
       "   'high': 0.5297157622739018},\n",
       "  'vgood': {'low': 0.014705882352941176,\n",
       "   'med': 0.014705882352941176,\n",
       "   'high': 0.9705882352941176},\n",
       "  'good': {'low': 0.013888888888888888,\n",
       "   'med': 0.5555555555555556,\n",
       "   'high': 0.4305555555555556}}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
